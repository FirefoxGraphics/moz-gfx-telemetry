{"cells":[{"cell_type":"code","source":["# The entry-point to this analysis is at the very bottom of this file.\n# Look for the call to DoUpdate()."],"metadata":{"collapsed":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":["dbutils.library.installPyPI(\"google-cloud-bigquery\", \"1.16.0\")\ndbutils.library.installPyPI(\"google-cloud-storage\", \"1.22.0\")\ndbutils.library.installPyPI(\"regex\")\ndbutils.library.install(\"dbfs:/eggs/bigquery_shim-0.5.8-py3.7.egg\")\ndbutils.library.restartPython()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from __future__ import division\nimport ujson as json\nimport numpy as np\nimport operator\nimport json, time, sys, os\nimport datetime\nfrom moztelemetry import get_one_ping_per_client\nfrom bigquery_shim import trends\n\ndef fmt_date(d):\n    return d.strftime(\"%Y%m%d\")\ndef jstime(d):\n    return time.mktime(d.timetuple())\ndef repartition(pipeline):\n    return pipeline.repartition(MaxPartitions).cache()\n\nMaxPartitions = sc.defaultParallelism * 4\n\n# Keep this small (0.00001) for fast backfill testing.\nWeeklyFraction = 0.003\n\n# Amount of days Telemetry keeps.\nMaxHistoryInDays = datetime.timedelta(days=210)\n\n# Bucket we'll drop files into on S3. If this is None, we won't attempt any\n# S3 uploads, and the analysis will start from scratch.\nS3_BUCKET = None\nGITHUB_REPO = 'https://raw.githubusercontent.com/FirefoxGraphics/moz-gfx-telemetry'\n\n# Going forward we only care about sessions from Firefox 53+, since it\n# is the first release to not support Windows XP and Vista, which disorts\n# our statistics.\nMinFirefoxVersion = '53'\n\n# List of jobs allowed to have a first-run (meaning no S3 content).\nBrandNewJobs = []\n\n# If true, backfill up to MaxHistoryInDays rather than the last update.\nForceMaxBackfill = False\n\n# Local directory on DBFS to store the trend data\nDBFS_PATH = 'gfx/trends'\n\n# The path used for Python IO\nABSOLUTE_PATH = '/dbfs/{0}'.format(DBFS_PATH)\n\n# The output path on S3\nS3_OUTPUT_BUCKET = 's3://telemetry-public-analysis-2/gfx/telemetry-data'"],"metadata":{"collapsed":false},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#dbutils.fs.rm(DBFS_PATH, recurse=True)\n\nprint('Creating local directory {0} on DBFS'.format(DBFS_PATH))\ndbutils.fs.mkdirs(DBFS_PATH)\n\nfrom os import walk\n\nf = []\nfor (dirpath, dirnames, filenames) in walk(ABSOLUTE_PATH):\n  f.extend(dirnames)\n  f.extend(filenames)\n  break\n\nprint('Current contents of {0}: {1}'.format(ABSOLUTE_PATH, f))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Use this block to temporarily change parameters above.\n# ForceMaxBackfill = True\n#WeeklyFraction = 0.00001\n#S3_BUCKET = None\n# MaxHistoryInDays = datetime.timedelta(days=30)\n#BrandNewJobs = []"],"metadata":{"collapsed":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":["if os.environ[\"DATABRICKS_RUNTIME_VERSION\"] and S3_BUCKET:\n  raise Exception(\"S3 sync is not supported on Databricks\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["ArchKey =               'environment/build/architecture'\nFxVersionKey =          'environment/build/version'\nWow64Key =              'environment/system/isWow64'\nCpuKey =                'environment/system/cpu'\nGfxAdaptersKey =        'environment/system/gfx/adapters'\nGfxFeaturesKey =        'environment/system/gfx/features'\nOSNameKey =             'environment/system/os/name'\nOSVersionKey =          'environment/system/os/version'\nOSServicePackMajorKey = 'environment/system/os/servicePackMajor'"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":["FirstValidDate = datetime.datetime.utcnow() - MaxHistoryInDays"],"metadata":{"collapsed":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Log spam eats up disk space, so we disable it.\ndef quiet_logs(sc):\n  logger = sc._jvm.org.apache.log4j\n  logger.LogManager.getLogger(\"org\").setLevel(logger.Level.ERROR)\n  logger.LogManager.getLogger(\"akka\").setLevel(logger.Level.ERROR)\nquiet_logs(sc)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# This is the entry-point to grabbing reduced, preformatted pings.\ndef FetchAndFormat(start_date, end_date):\n    pings = GetRawPings(start_date, end_date)\n    pings = get_one_ping_per_client(pings)\n    pings = pings.map(Validate)\n    pings = pings.filter(lambda p: p.get('valid', False) == True)\n    return pings.cache()\n    \ndef GetRawPings(start_date, end_date):\n    # WeeklyFraction ignored and baked into the included query\n    return trends.fetch_results(spark, start_date, end_date)\n\n# Transform each ping to make it easier to work with in later stages.\ndef Validate(p):\n    try:\n        name = p.get(OSNameKey) or 'w'\n        version = p.get(OSVersionKey) or '0'\n        if name == 'Linux':\n            p['OSVersion'] = None\n            p['OS'] = 'Linux'\n            p['OSName'] = 'Linux'\n        elif name == 'Windows_NT':\n            spmaj = p.get(OSServicePackMajorKey) or '0'\n            p['OSVersion'] = version + '.' + str(spmaj)\n            p['OS'] = 'Windows-' + version + '.' + str(spmaj)\n            p['OSName'] = 'Windows'\n        elif name == 'Darwin':\n            p['OSVersion'] = version\n            p['OS'] = 'Darwin-' + version\n            p['OSName'] = 'Darwin'\n        else:\n            p['OSVersion'] = version\n            p['OS'] = '{0}-{1}'.format(name, version)\n            p['OSName'] = name\n    except:\n        return p\n    \n    p['valid'] = True\n    return p"],"metadata":{"collapsed":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Profiler for debugging. Use in a |with| clause.\nclass Prof(object):\n    level = 0\n    \n    def __init__(self, name):\n        self.name = name\n    def __enter__(self):\n        self.sout('Starting {0}... '.format(self.name))\n        self.start = datetime.datetime.now()\n        Prof.level += 1\n        return None\n    def __exit__(self, type, value, traceback):\n        Prof.level -= 1\n        self.end = datetime.datetime.now()\n        self.sout('... {0}: {1}s'.format(self.name, (self.end - self.start).total_seconds()))\n    def sout(self, s):\n        sys.stdout.write(('##' * Prof.level) + ' ')\n        sys.stdout.write(s)\n        sys.stdout.write('\\n')\n        sys.stdout.flush()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Helpers.\ndef fix_vendor(vendorID):\n    if vendorID == u'Intel Open Source Technology Center':\n        return u'0x8086'\n    return vendorID\n\ndef get_vendor(ping):\n    try:\n        adapter = ping[GfxAdaptersKey][0]\n        return fix_vendor(adapter['vendorID'])\n    except:\n        return 'unknown'"],"metadata":{"collapsed":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# A TrendBase encapsulates the data needed to visualize a trend.\n# It has four functions:\n#    prepare    (download from cache)\n#    willUpdate (check if update is needed)\n#    update     (add analysis data for a week of pings)\n#    finish     (upload back to cache)\nclass TrendBase(object):\n    def __init__(self, name):\n        super(TrendBase, self).__init__()\n        self.name = '{0}-v2.json'.format(name)\n    \n    # Called before analysis starts.\n    def prepare(self):\n        print('Preparing {0}'.format(self.name))\n        return True\n    \n    # Called before querying pings for the week for the given date. Return\n    # false to indicate that this should no longer receive updates.\n    def willUpdate(self, date):\n        raise Exception('Return true or false')\n   \n    def update(self, pings, **kwargs):\n        raise Exception('NYI')\n        \n    def finish(self):\n        pass\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Given a list of trend objects, query weeks from the last sunday\n# and iterating backwards until no trend object requires an update.\ndef DoUpdate(trends):\n    root = TrendGroup('root', trends)\n    root.prepare()\n        \n    # Start each analysis slice on a Sunday.\n    latest = MostRecentSunday()\n    end = latest\n    \n    while True:\n        start = end - datetime.timedelta(7)\n        assert latest.weekday() == 6\n        \n        if not root.willUpdate(start):\n            break\n        \n        try:\n            with Prof('fetch {0}'.format(start)) as _:\n                pings = FetchAndFormat(start, end)\n        except:\n            if not ForceMaxBackfill:\n                raise\n        \n        with Prof('compute {0}'.format(start)) as _:\n            if not root.update(pings, start_date = start, end_date = end):\n                break\n            \n        end = start\n        \n    root.finish()\n    \ndef MostRecentSunday():\n    now = datetime.datetime.utcnow()\n    this_morning = datetime.datetime(now.year, now.month, now.day)\n    if this_morning.weekday() == 6:\n        return this_morning\n    diff = datetime.timedelta(0 - this_morning.weekday() - 1)\n    return this_morning + diff"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# A TrendGroup is a collection of TrendBase objects. It lets us\n# group similar trends together. For example, if five trends all\n# need to filter Windows pings, we can filter for Windows pings\n# once and cache the result, rather than redo the filter each\n# time.\n#\n# Trend groups keep an \"active\" list of trends that will probably\n# need another update. If any trend stops requesting data, it is\n# removed from the active list.\nclass TrendGroup(TrendBase):\n    def __init__(self, name, trends):\n        super(TrendGroup, self).__init__(name)\n        self.trends = trends\n        self.active = []\n    \n    def prepare(self):\n        self.trends = [trend for trend in self.trends if trend.prepare()]\n        self.active = self.trends[:]\n        return len(self.trends) > 0\n            \n    def willUpdate(self, date):\n        self.active = [trend for trend in self.active if trend.willUpdate(date)]\n        return len(self.active) > 0\n    \n    def update(self, pings, **kwargs):\n        pings = pings.cache()\n        self.active = [trend for trend in self.active if trend.update(pings, **kwargs)]\n        return len(self.active) > 0\n            \n    def finish(self):\n        for trend in self.trends:\n            trend.finish()\n            \n# A Trend object takes a new set of pings for a week's worth of data,\n# analyzes it, and adds the result to the trend set. Trend sets are\n# cached in S3 as JSON.\n#\n# If the latest entry in the cache covers less than a full week of\n# data, the entry is removed so that week can be re-queried.\nclass Trend(TrendBase):\n    def __init__(self, filename):\n        super(Trend, self).__init__(filename)\n        self.s3_path = os.path.join(S3_BUCKET, self.name) if S3_BUCKET else None\n        self.local_path = os.path.join(ABSOLUTE_PATH, self.name)\n        self.cache = None\n        self.lastFullWeek = None\n        self.newDataPoints = []\n        \n    def query(self, pings):\n        raise Exception('NYI')\n        \n    def willUpdate(self, date):\n        if date < FirstValidDate:\n            return False\n        if self.lastFullWeek is not None and date <= self.lastFullWeek:\n            return False\n        return True\n    \n    def prepare(self):\n        self.cache = self.fetch_json()\n        if self.cache is None:\n            self.cache = {\n                'created': jstime(datetime.datetime.utcnow()),\n                'trend': [],\n            }\n        \n        # Make sure trends are sorted in ascending order.\n        self.cache['trend'] = self.cache['trend'] or []            \n        self.cache['trend'] = sorted(self.cache['trend'], key = lambda o: o['start'])\n        \n        if len(self.cache['trend']) and not ForceMaxBackfill:\n            lastDataPoint = self.cache['trend'][-1]\n            lastDataPointStart = datetime.datetime.utcfromtimestamp(lastDataPoint['start'])\n            lastDataPointEnd = datetime.datetime.utcfromtimestamp(lastDataPoint['end'])\n            print(lastDataPoint, lastDataPointStart, lastDataPointEnd)\n            if lastDataPointEnd - lastDataPointStart < datetime.timedelta(7):\n                # The last data point had less than a full week, so we stop at the\n                # previous week, and remove the incomplete datapoint.\n                self.lastFullWeek = lastDataPointStart - datetime.timedelta(7)\n                self.cache['trend'].pop()\n            else:\n                # The last data point covered a full week, so that's our stopping\n                # point.\n                self.lastFullWeek = lastDataPointStart\n                print(self.lastFullWeek)\n        \n        return True\n    \n    # Optional hook - transform pings before querying.\n    def transformPings(self, pings):\n        return pings\n    \n    def update(self, pings, start_date, end_date, **kwargs):\n        with Prof('count {0}'.format(self.name)):\n            pings = self.transformPings(pings)\n            count = pings.count()\n        if count == 0:\n            print('WARNING: no pings in RDD')\n            return False\n        \n        with Prof('query {0} (count: {1})'.format(self.name, count)):\n            data = self.query(pings)\n        \n        self.newDataPoints.append({\n            'start': jstime(start_date),\n            'end': jstime(end_date),\n            'total': count,\n            'data': data,\n        })\n        return True\n            \n    def finish(self):\n        # If we're doing a maximum backfill, remove points from the cache that are\n        # after the least recent data point that we newly queried.\n        if ForceMaxBackfill and len(self.newDataPoints):\n            stopAt = self.newDataPoints[-1]['start']\n            lastIndex = None\n            for index, entry in enumerate(self.cache['trend']):\n                if entry['start'] >= stopAt:\n                    lastIndex = index\n                    break\n            if lastIndex is not None:\n                self.cache['trend'] = self.cache['trend'][:lastIndex]\n        \n        # Note: the backfill algorithm in DoUpdate() walks in reverse, so dates\n        # will be accumulated in descending order. The final list should be in\n        # ascending order, so we reverse.\n        self.cache['trend'] += self.newDataPoints[::-1]\n        \n        text = json.dumps(self.cache)\n\n        print(\"Writing file {0}\".format(self.local_path, text))\n        with open(self.local_path, 'w') as fp:\n            fp.write(text)\n\n        if self.s3_path:\n            try:\n                os.system(\"aws s3 cp {0} {1}\".format(self.local_path, self.s3_path))\n            except Exception as e:\n                print(\"Failed s3 upload: {0}\".format(e))\n            \n    def fetch_json(self):\n        print(\"Reading file {0}\".format(self.local_path))\n        if self.s3_path:\n            try:\n                os.system(\"aws s3 cp {0} {1}\".format(self.s3_path, self.local_path))\n                with open(self.local_path, 'r') as fp:\n                    return json.load(fp)\n                return None\n            except:\n                if self.name not in BrandNewJobs:\n                    raise\n                return None\n        else:\n            try:\n                with open(self.local_path, 'r') as fp:\n                    return json.load(fp)\n            except:\n                pass\n        return None"],"metadata":{"collapsed":false},"outputs":[],"execution_count":15},{"cell_type":"code","source":["class FirefoxTrend(Trend):\n    def __init__(self):\n        super(FirefoxTrend, self).__init__('trend-firefox')\n        \n    def query(self, pings, **kwargs):\n        def get_version(p):\n            v = p.get(FxVersionKey, None)\n            if v is None or not isinstance(v, str):\n                return 'unknown'\n            return v.split('.')[0]\n        return pings.map(lambda p: (get_version(p),)).countByKey()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":["class WindowsGroup(TrendGroup):\n    def __init__(self, trends):\n        super(WindowsGroup, self).__init__('Windows', trends)\n        \n    def update(self, pings, **kwargs):\n        pings = pings.filter(lambda p: p['OSName'] == 'Windows')\n        return super(WindowsGroup, self).update(pings, **kwargs)\n\nclass WinverTrend(Trend):\n    def __init__(self):\n        super(WinverTrend, self).__init__('trend-windows-versions')\n        \n    def query(self, pings):\n        return pings.map(lambda p: (p['OSVersion'],)).countByKey()\n    \nclass WinCompositorTrend(Trend):\n    def __init__(self):\n        super(WinCompositorTrend, self).__init__('trend-windows-compositors')\n        \n    def willUpdate(self, date):\n        # This metric didn't ship until Firefox 43.\n        if date < datetime.datetime(2015, 11, 15):\n            return False\n        return super(WinCompositorTrend, self).willUpdate(date)\n        \n    def query(self, pings):\n        return pings.map(lambda p: (self.get_compositor(p),)).countByKey()\n    \n    @staticmethod\n    def get_compositor(p):\n        features = p.get(GfxFeaturesKey, None)\n        if features is None:\n            return 'none'\n        return features.get('compositor', 'none')\n    \nclass WinArchTrend(Trend):\n    def __init__(self):\n        super(WinArchTrend, self).__init__('trend-windows-arch')\n        \n    def query(self, pings):\n        return pings.map(lambda p: (self.get_os_bits(p),)).countByKey()\n    \n    @staticmethod\n    def get_os_bits(p):\n        arch = p.get(ArchKey, 'unknown')\n        if arch == 'x86-64':\n            return '64'\n        elif arch == 'x86':\n            if p.get(Wow64Key, False):\n                return '32_on_64'\n            return '32'\n        return 'unknown'\n\n# This group restricts pings to Windows Vista+, and must be inside a\n# group that restricts pings to Windows.\nclass WindowsVistaPlusGroup(TrendGroup):\n    def __init__(self, trends):\n        super(WindowsVistaPlusGroup, self).__init__('Windows Vista+', trends)\n        \n    def update(self, pings, **kwargs):\n        pings = pings.filter(lambda p: not p['OSVersion'].startswith('5.1'))\n        return super(WindowsVistaPlusGroup, self).update(pings, **kwargs)\n\nclass Direct2DTrend(Trend):\n    def __init__(self):\n        super(Direct2DTrend, self).__init__('trend-windows-d2d')\n    \n    def query(self, pings):\n        return pings.map(lambda p: (self.get_d2d(p),)).countByKey()\n    \n    def willUpdate(self, date):\n        # This metric didn't ship until Firefox 43.\n        if date < datetime.datetime(2015, 11, 15):\n            return False\n        return super(Direct2DTrend, self).willUpdate(date)\n    \n    @staticmethod\n    def get_d2d(p):\n        try:\n            status = p[GfxFeaturesKey]['d2d']['status']\n            if status != 'available':\n                return status\n            return p[GfxFeaturesKey]['d2d']['version']\n        except:\n            return 'unknown'\n        \nclass Direct3D11Trend(Trend):\n    def __init__(self):\n        super(Direct3D11Trend, self).__init__('trend-windows-d3d11')\n    \n    def query(self, pings):\n        return pings.map(lambda p: (self.get_d3d11(p),)).countByKey()\n    \n    def willUpdate(self, date):\n        # This metric didn't ship until Firefox 43.\n        if date < datetime.datetime(2015, 11, 15):\n            return False\n        return super(Direct3D11Trend, self).willUpdate(date)\n    \n    @staticmethod\n    def get_d3d11(p):\n        try:\n            d3d11 = p[GfxFeaturesKey]['d3d11']\n            if d3d11['status'] != 'available':\n                return d3d11['status']\n            if d3d11.get('warp', False):\n                return 'warp'\n            return d3d11['version']\n        except:\n            return 'unknown'\n        \nclass WindowsVendorTrend(Trend):\n    def __init__(self):\n        super(WindowsVendorTrend, self).__init__('trend-windows-vendors')\n        \n    def query(self, pings):\n        return pings.map(lambda p: (get_vendor(p),)).countByKey()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Device generation trend - a little more complicated, since we download\n# the generation database to produce a mapping.\nclass DeviceGenTrend(Trend):\n    deviceMap = None\n    \n    def __init__(self, vendor, vendorName):\n        super(DeviceGenTrend, self).__init__('trend-windows-device-gen-{0}'.format(vendorName))\n        self.vendorBlock = None\n        self.vendorID = vendor\n        \n    def prepare(self):\n        # Grab the vendor -> device -> gen map.\n        if not DeviceGenTrend.deviceMap:\n            import requests\n            resp = requests.get('{0}/master/www/gfxdevices.json'.format(GITHUB_REPO))\n            DeviceGenTrend.deviceMap = resp.json()\n        self.vendorBlock = DeviceGenTrend.deviceMap[self.vendorID]\n        return super(DeviceGenTrend, self).prepare()\n    \n    def transformPings(self, pings):\n        return pings.filter(lambda p: get_vendor(p) == self.vendorID)\n        \n    def query(self, pings):\n        return pings.map(lambda p: (self.get_gen(p),)).countByKey()\n    \n    def get_gen(self, p):\n        adapter = p[GfxAdaptersKey][0]\n        deviceID = adapter.get('deviceID', 'unknown')\n        if deviceID not in self.vendorBlock:\n            return 'unknown'\n        return self.vendorBlock[deviceID][0]"],"metadata":{"collapsed":false},"outputs":[],"execution_count":18},{"cell_type":"code","source":["DoUpdate([\n    FirefoxTrend(),\n    WindowsGroup([\n        WinverTrend(),\n        WinCompositorTrend(),\n        WinArchTrend(),\n        WindowsVendorTrend(),\n        WindowsVistaPlusGroup([\n            Direct2DTrend(),\n            Direct3D11Trend(),\n        ]),\n        DeviceGenTrend(u'0x8086', 'intel'),\n        DeviceGenTrend(u'0x10de', 'nvidia'),\n        DeviceGenTrend(u'0x1002', 'amd'),\n    ])\n])"],"metadata":{"collapsed":false},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Copy the trend data from DBFS to S3\ndbutils.fs.cp(DBFS_PATH, S3_OUTPUT_BUCKET, recurse=True)\ndbutils.fs.ls(S3_OUTPUT_BUCKET)"],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.12","nbconvert_exporter":"python","file_extension":".py"},"name":"graphics-telemetry-trends","notebookId":237011,"kernelspec":{"display_name":"Python [default]","language":"python","name":"python2"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
